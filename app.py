# app.py
import streamlit as st
import os
from dotenv import load_dotenv

# --- LangChain ve Google GenerativeAI importlarÄ± ---
import google.generativeai as genai
from langchain_chroma import Chroma
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain


# ======================================================
# ===============  API ANAHTARI YÃ–NETÄ°MÄ°  ===============
# ======================================================

def get_api_key() -> str:
    """GOOGLE_API_KEY'i .env dosyasÄ±ndan alÄ±r."""
    load_dotenv()
    key = os.getenv("GOOGLE_API_KEY")
    if not key:
        st.error("âŒ GOOGLE_API_KEY ortam deÄŸiÅŸkeni bulunamadÄ±. LÃ¼tfen `.env` dosyanÄ±zÄ± kontrol edin.")
        st.stop()
    return key


def configure_genai(api_key: str):
    """Google GenAI istemcisini yapÄ±landÄ±rÄ±r."""
    try:
        genai.configure(api_key=api_key)
    except Exception as e:
        st.error(f"Google GenAI yapÄ±landÄ±rÄ±lÄ±rken hata oluÅŸtu: {e}")
        st.stop()


# ======================================================
# ================  RAG ZÄ°NCÄ°RÄ° YÃœKLEME  ===============
# ======================================================

@st.cache_resource(show_spinner=True)
def load_rag_chain():
    """
    Diskteki Chroma DB'den retriever oluÅŸturur,
    Google Embeddings ve Gemini LLM ile RAG zincirini kurar.
    """
    st.info("RAG zinciri yÃ¼kleniyor... LÃ¼tfen bekleyin.")

    # 1. Gemini embedding modeli
    try:
        embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
    except Exception as e:
        st.error(f"Embedding modeli yÃ¼klenemedi: {e}")
        st.stop()

    # 2. KayÄ±tlÄ± Chroma DB'yi yÃ¼kle
    persist_directory = "./chroma_db_kvkk"
    if not os.path.exists(persist_directory):
        st.error(f"Chroma veritabanÄ± bulunamadÄ±: {persist_directory}")
        st.error("LÃ¼tfen Ã¶nce `create_chroma_gemini.py` dosyasÄ±nÄ± Ã§alÄ±ÅŸtÄ±rarak veritabanÄ±nÄ± oluÅŸturun.")
        st.stop()

    try:
        vector_store = Chroma(
            persist_directory=persist_directory,
            embedding_function=embeddings
        )
    except Exception as e:
        st.error(f"Chroma yÃ¼klenirken hata oluÅŸtu: {e}")
        st.stop()

    # 3. Google Gemini LLM
    try:
        llm = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",  # hÄ±zlÄ± ve uygun maliyetli model
            temperature=0.55
        )
    except Exception as e:
        st.error(f"LLM yÃ¼klenemedi: {e}")
        st.stop()

    # 4. Retriever
    retriever = vector_store.as_retriever(search_kwargs={"k": 8})

    # 5. Sistem promptâ€™u
    system_prompt = (
        "Sen, KiÅŸisel Verilerin KorunmasÄ± Kanunu (KVKK) konusunda uzman bir yapay zekasÄ±n. "
        "CevaplarÄ±nÄ± Ã¶ncelikle sana verilen baÄŸlam (context) iÃ§indeki bilgilere dayandÄ±r. "
        "EÄŸer baÄŸlamda net bilgi yoksa, genel KVKK bilgisini kullanarak mantÄ±klÄ± ve aÃ§Ä±klayÄ±cÄ± bir cevap Ã¼ret. "
        "Yine de emin deÄŸilsen, 'BaÄŸlamda bu soruya doÄŸrudan yanÄ±t bulunamadÄ±.' de. "
        "CevaplarÄ±n TÃ¼rkÃ§e, kÄ±sa ve Ã¶ÄŸretici olmalÄ±dÄ±r.\n\n"
        "BaÄŸlam (Context):\n{context}"
    )


    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("human", "{input}")
    ])

    # 6. RAG zincirini oluÅŸtur
    question_answer_chain = create_stuff_documents_chain(llm, prompt)
    rag_chain = create_retrieval_chain(retriever, question_answer_chain)

    st.success("âœ… RAG zinciri baÅŸarÄ±yla yÃ¼klendi!")
    return rag_chain


# ======================================================
# ===================  STREAMLIT UI  ===================
# ======================================================

st.set_page_config(page_title="KVKK Chatbot", page_icon="ğŸ“„", layout="wide")
st.title("ğŸ“„ KVKK RAG Chatbot")
st.caption("KVKK belgelerinizle konuÅŸan yapay zekÃ¢ asistanÄ±")

# --- BaÅŸlatma ---
try:
    api_key = get_api_key()
    configure_genai(api_key)
    rag_chain = load_rag_chain()
except Exception as e:
    st.error(f"BaÅŸlatma hatasÄ±: {e}")
    st.stop()

# --- Sohbet geÃ§miÅŸi ---
if "messages" not in st.session_state:
    st.session_state.messages = [{
        "role": "assistant",
        "content": "Merhaba ğŸ‘‹ KVKK hakkÄ±ndaki sorularÄ±nÄ±zÄ± bana sorabilirsiniz."
    }]

# --- Mesaj geÃ§miÅŸini gÃ¶ster ---
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# --- KullanÄ±cÄ± giriÅŸi ---
if prompt := st.chat_input("Sorunuzu buraya yazÄ±n..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # --- Asistan cevabÄ± ---
    with st.chat_message("assistant"):
        with st.spinner("Belgeleri inceliyorum..."):
            try:
                response = rag_chain.invoke({"input": prompt})
                answer = (
                    response.get("answer") or
                    response.get("output_text") or
                    "Cevap Ã¼retilemedi."
                )
            except Exception as e:
                answer = f"âš ï¸ Hata oluÅŸtu: {e}"

            st.markdown(answer)

    st.session_state.messages.append({"role": "assistant", "content": answer})
